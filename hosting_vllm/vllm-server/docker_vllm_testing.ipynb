{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801fc6f-321c-4c2c-a9e6-b32a3d36fb13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub transformers==4.41.0 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb63a4c-e2eb-4f13-ae12-567708f4c649",
   "metadata": {},
   "source": [
    "#### Deploying Huggingface model with vLLM Docker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ea4a7a-ffd1-49e7-9ab2-b3e41e7c990a",
   "metadata": {},
   "source": [
    "This is the official sample from vLLM using docker. This will download the model Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4 directly\n",
    "\n",
    "https://docs.vllm.ai/en/v0.5.5/serving/deploying_with_docker.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c80a47-17ad-4c29-8d9e-81692e45c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the terminal and run the docker run command\n",
    "docker run --runtime nvidia --gpus all \\\n",
    "    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
    "    -p 8000:8000 \\\n",
    "    --ipc=host \\\n",
    "    vllm/vllm-openai:v0.6.3  \\\n",
    "    --model Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4 \\\n",
    "    --max-model-len 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c144f65e-2db4-41e5-a8fb-47875ab4fdc1",
   "metadata": {},
   "source": [
    "#### Deploying local model with vLLM Docker "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b013b3d7-1a5e-480f-9641-9ad5bdefd315",
   "metadata": {},
   "source": [
    "##### Download the model from Hugging Face "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c7f86fb-a2fe-4881-8063-a6ed7fcddbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e0a9b4c8ff4617a753694d2b75c74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247ea927e2604b24953c4626b54f5a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 10.5M/3.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6a5832bf534467ab08f820538476eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 10.5M/2.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "model_name = \"Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int4\"\n",
    "\n",
    "# - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "local_model_path = Path(\"./vllm-model\")\n",
    "local_model_path.mkdir(exist_ok=True)\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\", \"*.safetensors\"]\n",
    "\n",
    "# - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_model_path,\n",
    "    allow_patterns=allow_patterns,\n",
    "    revision=\"main\",  # Specify branch/tag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521ca85-2eeb-4d16-acae-133d58396c3c",
   "metadata": {},
   "source": [
    "##### Open the terminal and run the docker run command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c80b64-0313-49dc-992f-0e5bfd01c855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can run this with terminal \n",
    "docker run --runtime nvidia --gpus all \\\n",
    "    -v /home/ec2-user/SageMaker/docker-server/vllm-model/models--Qwen--Qwen2-VL-7B-Instruct-GPTQ-Int4/:/opt/ml/Qwen2-VL-7B-Instruct-GPTQ-Int4 \\\n",
    "    -p 8000:8000 \\\n",
    "    --ipc=host \\\n",
    "    vllm/vllm-openai:v0.6.3  \\\n",
    "    --model /opt/ml/Qwen2-VL-7B-Instruct-GPTQ-Int4/snapshots/dec510a35a3e9b6481b6427c7a08984df2402535 \\\n",
    "    --max-model-len 8192"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9030a-538d-4daa-ae58-0ede1c24990c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once loaded, you will see the log like following "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba200e-23c2-4d91-832f-570fea061e3b",
   "metadata": {},
   "source": [
    "INFO 11-18 04:21:11 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n",
    "\n",
    "INFO 11-18 04:22:13 metrics.py:345] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 44.5 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c660e88-d4f1-45de-89f5-639afadda1f2",
   "metadata": {},
   "source": [
    "### Testing the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3bbc04-5c26-4e30-8628-0657a3e251a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: OpenAI in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.54.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from OpenAI) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from OpenAI) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from OpenAI) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from OpenAI) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from OpenAI) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from OpenAI) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from OpenAI) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from OpenAI) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->OpenAI) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.5.0->OpenAI) (1.2.2)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->OpenAI) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpx<1,>=0.23.0->OpenAI) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->OpenAI) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->OpenAI) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->OpenAI) (2.23.4)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33263160-9a76-43cd-8488-6a339a982d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Modify OpenAI's API key and API base to use vLLM's API server.\n",
    "openai_api_key = \"EMPTY\"\n",
    "openai_api_base = \"http://localhost:8000/v1\"\n",
    "client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=openai_api_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158782d-9ed1-4faf-b227-d842fcd52ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion result: ChatCompletion(id='chat-305cffe00cc940c29dcde87a9eda181d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The image is a Management Discussion and Analysis (MD&A) document comparing the financial performance of a company for the years ended December 31, 2022 and December 31, 2021. The document presents the comparative figures for various financial metrics in millions of RMB. Here is a detailed breakdown of the contents:\\n\\n## Year Ended December 31, 2022 Compared to Year Ended December 31, 2021\\n\\n### Financial Metrics\\n- **Revenue**: 280,044.0 (2022) vs. 328,309.1 (2021)\\n- **Cost of Sales**: (232,466.8) (2022) vs. (270,048.2) (2021)\\n- **Gross Profit**: 47,577.2 (2022) vs. 58,260.9 (2021)\\n- **Research and Development Expenses**: (16,028.1) (2022) vs. (13,167.1) (2021)\\n- **Selling and Marketing Expenses**: (21,323.3) (2022) vs. (20,980.8) (2021)\\n- **Administrative Expenses**: (5,113.9) (2022) vs. (4,738.9) (2021)\\n- **Fair Value Changes on Investments measured at fair value through profit or loss**: (1,662.0) (2022) vs. 8,132.1 (2021)\\n- **Share of net (losses)/profits of investments accounted for using the equity method**: (400.1) (2022) vs. 275.0 (2021)\\n- **Other Income**: 1,135.5 (2022) vs. 826.9 (2021)\\n- **Other Losses, net**: (1,368.8) (2022) vs. (2,579.5) (2021)\\n- **Operating Profit**: 2,816.5 (2022) vs. 26,028.6 (2021)\\n- **Finance Income/(Costs), net**: 1,117.5 (2022) vs. (1,611.6) (2021)\\n- **Profit before Income Tax**: 3,934.0 (2022) vs. 24,417.0 (2021)\\n- **Income Tax Expenses**: (1,431.4) (2022) vs. (5,133.8) (2021)\\n- **Profit for the Year**: 2,502.6 (2022) vs. 19,283.2 (2021)\\n\\n### Additional Information\\n- **Non-IFRS Measure: Adjusted Net Profit**: 8,518.0 (2022) vs. 22,039.5 (2021)\\n\\nThis document provides a comprehensive overview of the company's financial performance, focusing on key financial metrics that reflect the company's operational efficiency, profitability, and financial health. The year-over-year comparison highlights changes in various financial aspects, which can be crucial for stakeholders in assessing the company's performance and making informed decisions.\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1731932512, model='/opt/ml/Qwen2-VL-7B-Instruct-GPTQ-Int4/snapshots/dec510a35a3e9b6481b6427c7a08984df2402535', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=809, prompt_tokens=1968, total_tokens=2777, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Replace with your image path\n",
    "test_image = \"income_statement.jpg\"\n",
    "base64_image = encode_image(test_image)\n",
    "\n",
    "url = \"http://localhost:8000/invocations\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Please generate accurate HTML code that represents the table structure shown in input image, including any merged cells.\"\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/png;base64,{base64_image}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1024\n",
    "}\n",
    "\n",
    "completion = client.chat.completions.create(model=\"/opt/ml/Qwen2-VL-7B-Instruct-GPTQ-Int4/snapshots/dec510a35a3e9b6481b6427c7a08984df2402535\",\n",
    "                                       messages=request[\"messages\"],\n",
    "                                       max_tokens=request[\"max_tokens\"])\n",
    "\n",
    "\n",
    "print(\"Completion result:\", completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2c9ee-0f4a-4f09-ae65-d4e3149fb44f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
